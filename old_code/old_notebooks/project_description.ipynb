{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: a coupled variational autoencoder for multimodal data\n",
    "\n",
    "### Introduction \n",
    "\n",
    "Single-cell genomics allows to profile not only a single data modality (gene expression, chromatin accessibility,...) but multiple modalities at once from the same cell (\"multimodal data\"). This allows to gain a better understanding of the cellular state by looking at different biological processes within the same cell, and to characterize better the cell state. This includes for example profiling of gene expression and chromatin accessibility (scRNA-seq and scATAC-seq) or gene expression and DNA methylation. Another type of multimodal data is [CITE-seq](https://cite-seq.com/), in which gene expression is simultaneously profiled with the expression of proteins expressed at the surface of the cell. This is for example used used to profile immune cells, which are often characterized by a combination of surface proteins.\n",
    "\n",
    "### Goal\n",
    "\n",
    "The goal of this project is to build a VAE based model to couple the two data modalities from CITE-seq data, i.e. gene expression and durface protein expression. The aim is to compare the embedding obtained from the joint processing of the two data modalities with the embedding obtained from each modality individually. This should be done in a two step process:\n",
    "\n",
    "1. build single VAEs for each modality, and perform the embedding in each modality individually (RNA-seq and protein expression)\n",
    "2. couple the two VAE to obtain a joint embedding.\n",
    "\n",
    "Regarding this second point, multiple approaches can be used, listed here in order of increasing complexity: (1) the concatenation of hidden layers to learn a shared latent space between the modalities (as is done for example in [OmiVAE for bulk data](https://arxiv.org/abs/1908.06278)), (2) training cross-modality VAEs as is done in [BABEL](https://www.pnas.org/doi/full/10.1073/pnas.2023070118) or (3) learning a shared latent space using an adversarial approach (see [here](https://www.nature.com/articles/s41467-020-20249-2)).\n",
    "\n",
    "We expect you to compare the embeddings obtained either by the single-modality VAE and the multimodal VAE and discuss the differences. For example, are certain cell population better separated in the multimodal model compare to the single modalities? Also compare the behavior of the single models in terms of loss, ...\n",
    "\n",
    "### Data and model\n",
    "\n",
    "The data to be used is a CITE-seq dataset provided by 10x genomics. Check the tutorial of [scanpy of CITE-seq](https://scanpy-tutorials.readthedocs.io/en/multiomics/cite-seq/pbmc5k.html) to see how to obtain the dataset and convert it into the AnnData format.\n",
    "Alternatively, you can also use the CITE-seq dataset provided [as part of the NeurIPS21 competition](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE194122). This is the [link](https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE194122&format=file&file=GSE194122%5Fopenproblems%5Fneurips2021%5Fcite%5FBMMC%5Fprocessed%2Eh5ad%2Egz) to the CITE-seq data in anndata format.\n",
    "\n",
    "You can use snippets of code from the lab of week 7 (C. Herrmann) on VAE in genomics as a starting point for single-cell VAEs, and modify the architecture as needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
