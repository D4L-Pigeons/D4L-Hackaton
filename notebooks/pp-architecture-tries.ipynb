{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../src\")\n",
    "\n",
    "from utils import data_utils\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import anndata as ad\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from argparse import Namespace\n",
    "from typing import Dict, Optional, Tuple, Union\n",
    "\n",
    "import anndata as ad\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.distributions as td\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from anndata import AnnData\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from torch import Tensor\n",
    "\n",
    "from models.building_blocks import Block, ResidualBlock, ShortcutBlock\n",
    "from models.ModelBase import ModelBase\n",
    "from utils.data_utils import get_dataloader_from_anndata, get_dataset_from_anndata\n",
    "from utils.loss_utils import (\n",
    "    adt_reconstruction_loss,\n",
    "    gex_reconstruction_loss,\n",
    "    kld_stdgaussian,\n",
    ")\n",
    "from utils.paths import LOGS_PATH\n",
    "\n",
    "\n",
    "class OmiAE(pl.LightningModule):\n",
    "    def __init__(self, cfg: Namespace):\n",
    "        super(OmiAE, self).__init__()\n",
    "        self.assert_cfg(cfg)\n",
    "        self.cfg = cfg\n",
    "        self.fstmod_in = ShortcutBlock(\n",
    "            input_size=cfg.first_modality_dim,\n",
    "            output_size=cfg.first_modality_embedding_dim,\n",
    "            hidden_size=cfg.first_modality_hidden_dim,\n",
    "            batch_norm=cfg.batch_norm,\n",
    "        )\n",
    "        self.sndmod_in = ShortcutBlock(\n",
    "            input_size=cfg.second_modality_dim,\n",
    "            output_size=cfg.second_modality_embedding_dim,\n",
    "            hidden_size=cfg.second_modality_hidden_dim,\n",
    "            batch_norm=cfg.batch_norm,\n",
    "        )\n",
    "        self.encoder = nn.Sequential(\n",
    "            Block(\n",
    "                input_size=cfg.first_modality_embedding_dim\n",
    "                + cfg.second_modality_embedding_dim,\n",
    "                output_size=cfg.encoder_out_dim,\n",
    "                hidden_size=cfg.encoder_hidden_dim,\n",
    "                batch_norm=cfg.batch_norm,\n",
    "            ),\n",
    "            nn.SiLU(),\n",
    "            ResidualBlock(\n",
    "                input_size=cfg.encoder_out_dim,\n",
    "                hidden_size=cfg.encoder_hidden_dim,\n",
    "                batch_norm=cfg.batch_norm,\n",
    "            ),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            ShortcutBlock(\n",
    "                input_size=cfg.decoder_in_dim,\n",
    "                output_size=cfg.decoder_in_dim * 2,\n",
    "                hidden_size=cfg.decoder_hidden_dim,\n",
    "                batch_norm=cfg.batch_norm,\n",
    "            ),\n",
    "            nn.SiLU(),\n",
    "            Block(\n",
    "                input_size=cfg.decoder_in_dim * 2,\n",
    "                output_size=cfg.first_modality_embedding_dim\n",
    "                + cfg.second_modality_embedding_dim,\n",
    "                hidden_size=cfg.decoder_hidden_dim,\n",
    "                batch_norm=cfg.batch_norm,\n",
    "            ),\n",
    "        )\n",
    "        self.fstmod_out = ShortcutBlock(\n",
    "            input_size=cfg.first_modality_embedding_dim,\n",
    "            output_size=cfg.first_modality_dim,\n",
    "            hidden_size=cfg.first_modality_hidden_dim,\n",
    "            batch_norm=cfg.batch_norm,\n",
    "        )\n",
    "        self.sndmod_out = ShortcutBlock(\n",
    "            input_size=cfg.second_modality_embedding_dim,\n",
    "            output_size=cfg.second_modality_dim,\n",
    "            hidden_size=cfg.second_modality_hidden_dim,\n",
    "            batch_norm=cfg.batch_norm,\n",
    "        )\n",
    "        if cfg.classification_head:\n",
    "            self.classification_head = Block(\n",
    "                input_size=cfg.decoder_in_dim,\n",
    "                output_size=cfg.num_classes,\n",
    "                hidden_size=cfg.num_classes * 2,\n",
    "                batch_norm=cfg.batch_norm,\n",
    "            )\n",
    "\n",
    "    def _encode(self, x_fst: Tensor, x_snd: Tensor) -> Tensor:\n",
    "        x_fst = self.fstmod_in(x_fst)\n",
    "        # print(\"encode 0 passed\")\n",
    "        x_snd = self.sndmod_in(x_snd)\n",
    "        # print(\"encode 1 passed\")\n",
    "        encoder_out = self.encoder(torch.cat([x_fst, x_snd], dim=-1))\n",
    "\n",
    "        return encoder_out\n",
    "\n",
    "    def _decode(self, z: Tensor) -> Tuple[Tensor]:\n",
    "        x_fst, x_snd = self.decoder(z).split(\n",
    "            [\n",
    "                self.cfg.first_modality_embedding_dim,\n",
    "                self.cfg.second_modality_embedding_dim,\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "        # print(\"decode 0 passed\")\n",
    "        x_fst = self.fstmod_out(x_fst)\n",
    "        # print(\"decode 1 passed\")\n",
    "        x_snd = self.sndmod_out(x_snd)\n",
    "        # print(\"decode 2 passed\")\n",
    "\n",
    "        return x_fst, x_snd\n",
    "\n",
    "    def _classification_processing(\n",
    "        self, latent_representation: Tensor, y: Tensor, compute_accuracy: bool\n",
    "    ) -> Tuple[Tensor, Optional[Tensor], Optional[Tensor]]:\n",
    "        logits = self.classification_head(latent_representation)\n",
    "        c_loss = F.cross_entropy(logits, y, weight=self.cfg.class_weights)\n",
    "        if compute_accuracy:\n",
    "            acc = ((torch.argmax(logits, dim=1) == y).float().mean()).item()\n",
    "            bac = balanced_accuracy_score(\n",
    "                y.cpu().numpy(), torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            )\n",
    "            return c_loss, acc, bac\n",
    "\n",
    "        return c_loss, None, None\n",
    "\n",
    "    def forward(\n",
    "        self, batch: Tuple[Tensor], compute_accuracy: bool = False\n",
    "    ) -> Tuple[Tensor, Dict[str, float]]:\n",
    "        metrics = {}\n",
    "        total_loss = 0.0\n",
    "        x_fst, x_snd, *packed_labels_potentially = batch\n",
    "        assert isinstance(x_fst, Tensor), TypeError(\n",
    "            f\"x_fst must be a Tensor, got {type(x_fst)} instead.\"\n",
    "        )\n",
    "        assert isinstance(x_snd, Tensor), TypeError(\n",
    "            f\"x_snd must be a Tensor, got {type(x_snd)} instead.\"\n",
    "        )\n",
    "        z = self._encode(x_fst, x_snd)\n",
    "        assert isinstance(z, Tensor), TypeError(\n",
    "            f\"z must be a Tensor, got {type(z)} instead.\"\n",
    "        )\n",
    "        x_fst_hat, x_snd_hat = self._decode(z)\n",
    "        assert isinstance(x_fst_hat, Tensor), TypeError(\n",
    "            f\"x_fst_hat must be a Tensor, got {type(x_fst_hat)} instead.\"\n",
    "        )\n",
    "        assert isinstance(x_snd_hat, Tensor), TypeError(\n",
    "            f\"x_snd_hat must be a Tensor, got {type(x_snd_hat)} instead.\"\n",
    "        )\n",
    "        recon_loss = F.mse_loss(x_fst_hat, x_fst) + F.mse_loss(x_snd_hat, x_snd)\n",
    "        metrics[\"recon_loss\"] = recon_loss.item()\n",
    "        total_loss += self.cfg.recon_loss_coef * recon_loss\n",
    "\n",
    "        if self.cfg.classification_head:\n",
    "            c_loss, acc, bac = self._classification_processing(\n",
    "                z, *packed_labels_potentially, compute_accuracy\n",
    "            )\n",
    "            metrics[\"class_loss\"] = c_loss.item()\n",
    "            if acc is not None:\n",
    "                metrics[\"acc\"] = acc\n",
    "                metrics[\"bac\"] = bac\n",
    "            total_loss += self.cfg.c_loss_coef * c_loss\n",
    "\n",
    "        return total_loss, metrics\n",
    "\n",
    "    def training_step(self, batch: Tensor) -> Tensor:\n",
    "        loss, loss_components = self(batch)\n",
    "        self.log(\"Train loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        for k, v in loss_components.items():\n",
    "            self.log(f\"Train {k}\", v, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch: Tensor) -> Tensor:\n",
    "        loss, loss_components = self(batch)\n",
    "        self.log(\"Val loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        for k, v in loss_components.items():\n",
    "            self.log(f\"Val {k}\", v, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def _predict(self, x: Tensor) -> Tensor:\n",
    "        if not self.classification_head:\n",
    "            raise ValueError(\"Model does not have a classification head\")\n",
    "        mu, _ = self._encode(x)\n",
    "        logits = self.classification_head(mu)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def _predict_proba(self, data: AnnData):\n",
    "        if not self.classification_head:\n",
    "            raise ValueError(\"Model does not have a classification head\")\n",
    "        mu, _ = self._encode(data)\n",
    "        logits = self.classification_head(mu)\n",
    "        return torch.softmax(logits, dim=1)\n",
    "\n",
    "    def _get_decoder_jacobian(self, z: Tensor) -> Tensor:\n",
    "        return torch.autograd.functional.jacobian(self.decoder, z)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.cfg.lr)\n",
    "\n",
    "    def assert_cfg(self, cfg: Namespace) -> None:\n",
    "        default_cfg = {\n",
    "            \"max_epochs\": 5,\n",
    "            \"log_every_n_steps\": 1,\n",
    "            \"first_modality_hidden_dim\": 50,\n",
    "            \"second_modality_hidden_dim\": 10,\n",
    "            \"first_modality_embedding_dim\": 50,\n",
    "            \"second_modality_embedding_dim\": 10,\n",
    "            \"encoder_hidden_dim\": 5000,\n",
    "            \"encoder_out_dim\": 40,\n",
    "            \"decoder_in_dim\": 10,\n",
    "            \"decoder_hidden_dim\": 20,\n",
    "            \"recon_loss_coef\": 1,\n",
    "            \"c_loss_coef\": 1,\n",
    "            \"kld_loss_coef\": 1,\n",
    "            \"lr\": 0.001,\n",
    "        }\n",
    "        for attr, default_value in default_cfg.items():\n",
    "            if not hasattr(cfg, attr):\n",
    "                setattr(cfg, attr, default_value)\n",
    "                print(f\"{attr} set as {default_value}\")\n",
    "\n",
    "\n",
    "class OmiGMPriorProbabilisticAE(OmiAE):\n",
    "    def __init__(self, cfg: Namespace):\n",
    "        super(OmiGMPriorProbabilisticAE, self).__init__(cfg)\n",
    "        self.assert_cfg(cfg)\n",
    "        self.component_logits = nn.Parameter(\n",
    "            data=torch.zeros(size=(cfg.no_components,)), requires_grad=True\n",
    "        )\n",
    "        self.means = nn.Parameter(\n",
    "            torch.randn(cfg.no_components, cfg.latent_dim), requires_grad=True\n",
    "        )\n",
    "        # STDs of GMM\n",
    "        self.register_buffer(\n",
    "            \"stds\", cfg.components_std * torch.ones(cfg.no_components, cfg.latent_dim)\n",
    "        )\n",
    "\n",
    "    def _var_transformation(self, logvar: Tensor) -> Tensor:\n",
    "        return torch.exp(0.5 * logvar)\n",
    "\n",
    "    def forward(\n",
    "        self, batch: Tuple[Tensor], compute_accuracy: bool = False\n",
    "    ) -> Tuple[Tensor]:\n",
    "        metrics: Dict[str, float] = {}\n",
    "        total_loss: Tensor = 0.0\n",
    "        (\n",
    "            x_fst,\n",
    "            x_snd,\n",
    "            labels,\n",
    "        ) = batch  # ASSUMPTION THAT ALL LABELS ARE AVAILABLE (the extension to the mix of alebeled + unlabeled is not difficuls, but it is not implemented here as it may not be necessary for the task at hand)\n",
    "        labels = torch.bernoulli(torch.ones_like(labels) * 0.5).long()\n",
    "        # assert (\n",
    "        #     False\n",
    "        # ), \"The labels are random for now, this should be changed to the actual labels\"\n",
    "        z_means, z_stds = self._encode(x_fst, x_snd).chunk(2, dim=1)\n",
    "        z_stds = self._var_transformation(z_stds)\n",
    "        normal_rv = self._make_normal_rv(z_means, z_stds)\n",
    "        entropy_per_batch_sample = normal_rv.entropy().sum(dim=1).unsqueeze(0)  # [1, B]\n",
    "        assert entropy_per_batch_sample.shape == (1, x_fst.shape[0]), AssertionError(\n",
    "            f\"Entropy shape is {entropy_per_batch_sample.shape}, expected {(1, x_fst.shape[0])}\"\n",
    "        )\n",
    "        z_sample = normal_rv.rsample(\n",
    "            sample_shape=(self.cfg.no_latent_samples,)\n",
    "        ).unsqueeze(\n",
    "            2\n",
    "        )  # [K, B, 1, latent_dim]\n",
    "        assert z_sample.shape == (\n",
    "            self.cfg.no_latent_samples,\n",
    "            x_fst.shape[0],\n",
    "            1,\n",
    "            self.cfg.latent_dim,\n",
    "        ), AssertionError(\n",
    "            f\"z_sample shape is {z_sample.shape}, expected {(self.cfg.no_latent_samples, x_fst.shape[0], 1, self.cfg.latent_dim)}\"\n",
    "        )\n",
    "\n",
    "        gmm = self._make_gmm()\n",
    "        per_component_logprob = gmm.component_distribution.log_prob(\n",
    "            z_sample\n",
    "        )  # [K, B, no_components]\n",
    "        assert per_component_logprob.shape == (\n",
    "            self.cfg.no_latent_samples,\n",
    "            x_fst.shape[0],\n",
    "            self.cfg.no_components,\n",
    "        ), AssertionError(\n",
    "            f\"per_component_logprob shape is {per_component_logprob.shape}, expected {(self.cfg.no_latent_samples, x_fst.shape[0], self.cfg.no_components)}\"\n",
    "        )\n",
    "        component_indicator = torch.arange(self.cfg.no_components).unsqueeze(0).repeat(\n",
    "            (x_fst.shape[0], 1)\n",
    "        ) == labels.unsqueeze(1)\n",
    "        assert component_indicator.shape == (\n",
    "            x_fst.shape[0],\n",
    "            self.cfg.no_components,\n",
    "        ), AssertionError(\n",
    "            f\"component_indicator shape is {component_indicator.shape}, expected {(x_fst.shape[0], self.cfg.no_components)}\"\n",
    "        )\n",
    "        gmm_likelihood_per_k = per_component_logprob[:, component_indicator]  # [K, B]\n",
    "        assert gmm_likelihood_per_k.shape == (\n",
    "            self.cfg.no_latent_samples,\n",
    "            x_fst.shape[0],\n",
    "        ), AssertionError(\n",
    "            f\"gmm_likelihood_per_k shape is {gmm_likelihood_per_k.shape}, expected {(self.cfg.no_latent_samples, x_fst.shape[0])}\"\n",
    "        )\n",
    "\n",
    "        x_fst_hat, x_snd_hat = self._decode(z_sample.squeeze(2))\n",
    "        recon_loss_per_k = F.mse_loss(\n",
    "            x_fst_hat, x_fst.repeat(self.cfg.no_latent_samples, 1, 1), reduction=\"none\"\n",
    "        ).mean(dim=-1) + F.mse_loss(\n",
    "            x_snd_hat, x_snd.repeat(self.cfg.no_latent_samples, 1, 1), reduction=\"none\"\n",
    "        ).mean(\n",
    "            dim=-1\n",
    "        )  # [K, B]\n",
    "        assert recon_loss_per_k.shape == (\n",
    "            self.cfg.no_latent_samples,\n",
    "            x_fst.shape[0],\n",
    "        ), AssertionError(\n",
    "            f\"recon_loss_per_k shape is {recon_loss_per_k.shape}, expected {(self.cfg.no_latent_samples, x_fst.shape[0])}\"\n",
    "        )\n",
    "\n",
    "        if self.cfg.no_latent_samples > 1:  # IWAE with no_latent_samples latent samples\n",
    "            total_loss = -torch.logsumexp(\n",
    "                # gmm_likelihood_per_k + recon_loss_per_k + entropy_per_batch_sample,\n",
    "                self.cfg.gmm_likelihood_loss_coef * gmm_likelihood_per_k\n",
    "                + self.cfg.entropy_loss_coef * entropy_per_batch_sample\n",
    "                + self.cfg.recon_loss_coef * recon_loss_per_k,\n",
    "                dim=0,\n",
    "            ).mean()\n",
    "        else:  # IWAE reduces to VAE with one latent sample\n",
    "            total_loss = -(\n",
    "                gmm_likelihood_per_k + recon_loss_per_k + entropy_per_batch_sample\n",
    "            ).mean()\n",
    "\n",
    "        metrics[\"entropy\"] = entropy_per_batch_sample.mean().item()\n",
    "        metrics[\"gmm_likelihood\"] = gmm_likelihood_per_k.mean().item()\n",
    "        metrics[\"recon_loss\"] = recon_loss_per_k.mean().item()\n",
    "\n",
    "        if self.cfg.classification_head:\n",
    "            c_loss, acc, bac = self._classification_processing(\n",
    "                z_means, labels, compute_accuracy\n",
    "            )\n",
    "            metrics[\"class_loss\"] = c_loss.item()\n",
    "            if acc is not None:\n",
    "                metrics[\"acc\"] = acc\n",
    "                metrics[\"bac\"] = bac\n",
    "            total_loss += self.cfg.c_loss_coef * c_loss\n",
    "\n",
    "        return total_loss, metrics\n",
    "\n",
    "    def _make_normal_rv(self, mu: Tensor, logvar: Tensor):\n",
    "        return td.Normal(mu, self._var_transformation(logvar))\n",
    "\n",
    "    def _make_gmm(self):\n",
    "        categorical = td.Categorical(logits=self.component_logits)\n",
    "        comp = td.Independent(\n",
    "            td.Normal(self.means, self.stds), reinterpreted_batch_ndims=1\n",
    "        )\n",
    "        return td.MixtureSameFamily(categorical, comp)\n",
    "\n",
    "    def assert_cfg(self, cfg: Namespace) -> None:\n",
    "        super(OmiGMPriorProbabilisticAE, self).assert_cfg(cfg)\n",
    "        default_cfg = {\n",
    "            \"no_components\": 2,\n",
    "            \"components_std\": 1,\n",
    "            \"no_latent_samples\": 16,\n",
    "            \"gmm_likelihood_loss_coef\": 0.1,\n",
    "            \"entropy_loss_coef\": 0.1,\n",
    "        }\n",
    "        for attr, default_value in default_cfg.items():\n",
    "            if not hasattr(cfg, attr):\n",
    "                setattr(cfg, attr, default_value)\n",
    "                print(f\"{attr} set as {default_value}\")\n",
    "\n",
    "        assert cfg.latent_dim * 2 == cfg.encoder_out_dim, ValueError(\n",
    "            \"The latent dimension must be twice the encoder output dimension\"\n",
    "        )\n",
    "\n",
    "\n",
    "# class OmiHierarchicalGMPriorProbabilisticAE(OmiGMPriorProbabilisticAE): :) to be continued...\n",
    "\n",
    "# class M1M2 MODEL :) to be continued...\n",
    "\n",
    "# CLASS NALEÅšNIK MODEL IF IT EVEN DIFFERES FROM THE ABOVE MODELS :) to be continued...\n",
    "\n",
    "_OMIVAE_IMPLEMENTATIONS = {\n",
    "    \"OmiAE\": OmiAE,\n",
    "    \"OmiGMPriorProbabilisticAE\": OmiGMPriorProbabilisticAE,\n",
    "}\n",
    "\n",
    "\n",
    "class OmiModel(ModelBase):\n",
    "    def __init__(self, cfg: Namespace):\n",
    "        super(OmiModel, self).__init__()\n",
    "        self.assert_cfg(cfg)\n",
    "        self.cfg = cfg\n",
    "        self.model = _OMIVAE_IMPLEMENTATIONS[cfg.omivae_implementation](cfg)\n",
    "        self.trainer = pl.Trainer(\n",
    "            max_epochs=cfg.max_epochs,\n",
    "            log_every_n_steps=cfg.log_every_n_steps,\n",
    "            logger=pl.loggers.TensorBoardLogger(\n",
    "                LOGS_PATH, name=cfg.omivae_implementation\n",
    "            ),\n",
    "            callbacks=(\n",
    "                [\n",
    "                    pl.callbacks.EarlyStopping(\n",
    "                        monitor=\"val_loss\",\n",
    "                        min_delta=cfg.min_delta,\n",
    "                        patience=cfg.patience,\n",
    "                        verbose=False,\n",
    "                        mode=\"min\",\n",
    "                    )\n",
    "                ]\n",
    "                if self.cfg.early_stopping\n",
    "                else None\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def train(self, train_data: AnnData, val_data: AnnData = None) -> None:\n",
    "        self.trainer.fit(\n",
    "            model=self.model,\n",
    "            train_dataloaders=get_dataloader_from_anndata(\n",
    "                train_data,\n",
    "                self.cfg.first_modality_dim,\n",
    "                self.cfg.second_modality_dim,\n",
    "                self.cfg.batch_size,\n",
    "                shuffle=True,\n",
    "                include_class_labels=self.cfg.classification_head\n",
    "                or self.cfg.include_class_labels,\n",
    "            ),\n",
    "            val_dataloaders=(\n",
    "                get_dataset_from_anndata(\n",
    "                    val_data,\n",
    "                    self.cfg.first_modality_dim,\n",
    "                    self.cfg.second_modality_dim,\n",
    "                    include_class_labels=self.cfg.classification_head,\n",
    "                )\n",
    "                if val_data is not None\n",
    "                else None\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def predict(self, data: AnnData):\n",
    "        pass\n",
    "\n",
    "    def predict_proba(self, data: AnnData):\n",
    "        pass\n",
    "\n",
    "    def save(self, file_path: str):\n",
    "        save_path = file_path + \".ckpt\"\n",
    "        torch.save(self.model.state_dict(), save_path)\n",
    "        return save_path\n",
    "\n",
    "    def load(self, file_path: str):\n",
    "        load_path = file_path + \".ckpt\"\n",
    "        self.model.load_state_dict(torch.load(load_path))\n",
    "\n",
    "    def assert_cfg(self, cfg: Namespace) -> None:\n",
    "        self.assert_cfg_general(cfg)\n",
    "\n",
    "        assert hasattr(cfg, \"omivae_implementation\"), AttributeError(\n",
    "            'cfg does not have the attribute \"omivae_implementation\"'\n",
    "        )\n",
    "        assert cfg.omivae_implementation in _OMIVAE_IMPLEMENTATIONS, ValueError(\n",
    "            f\"Invalid OmiVAE implementation: {cfg.omivae_implementation}\"\n",
    "        )\n",
    "        assert hasattr(cfg, \"output_modelling_type\"), AttributeError(\n",
    "            'cfg does not have the attribute \"output_modelling_type\"'\n",
    "        )\n",
    "        assert cfg.output_modelling_type in [\n",
    "            \"mse_direct_reconstruction\",\n",
    "            \"ll_neg_binomial\",\n",
    "        ], ValueError(\n",
    "            f\"Invalid output modelling type: {cfg.output_modelling_type}. Must be one of ['mse_direct_reconstruction', 'll_neg_binomial']\"\n",
    "        )\n",
    "\n",
    "        if not hasattr(cfg, \"early_stopping\"):\n",
    "            setattr(cfg, \"early_stopping\", True)\n",
    "            print(f\"early_stopping set as True\")\n",
    "        if cfg.early_stopping:\n",
    "            if not hasattr(cfg, \"min_delta\"):\n",
    "                setattr(cfg, \"min_delta\", 0.001)\n",
    "                print(f\"min_delta set as 0.001\")\n",
    "            if not hasattr(cfg, \"patience\"):\n",
    "                setattr(cfg, \"patience\", 5)\n",
    "                print(f\"patience set as 5\")\n",
    "\n",
    "\n",
    "# class OmiVAE(OmiAE):\n",
    "\n",
    "# class OmiIWAE(OmiAE):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.paths import CONFIG_PATH\n",
    "import yaml\n",
    "\n",
    "def load_config(args) -> argparse.Namespace:\n",
    "    with open(CONFIG_PATH / args.method / f\"{args.config}.yaml\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "    return argparse.Namespace(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(method=\"OmiVAE\", config=\"pp-experimental\")\n",
    "cfg = load_config(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(eksperyment=5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = data_utils.load_anndata(\n",
    "    mode='train',\n",
    "    normalize=True,\n",
    "    remove_batch_effect=True,\n",
    "    add_hierarchy=True\n",
    ")\n",
    "train_dataloader = data_utils.get_dataloader_from_anndata(\n",
    "    adata,\n",
    "    batch_size=cfg.batch_size,\n",
    "    target_hierarchy_level=cfg.target_hierarchy_level\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
