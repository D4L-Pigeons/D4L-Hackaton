{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_utils import load_anndata\n",
    "from train import load_config, create_model\n",
    "from models.babel import BabelModel\n",
    "import json\n",
    "import yaml\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from argparse import Namespace\n",
    "from itertools import cycle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import torch\n",
    "from pytorch_lightning import LightningModule\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    auc,\n",
    "    normalized_mutual_info_score,\n",
    "    roc_curve,\n",
    "    silhouette_score,\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from models.building_blocks import Block\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adamb\\miniconda3\\Lib\\site-packages\\anndata\\_core\\anndata.py:1820: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n"
     ]
    }
   ],
   "source": [
    "test_data = load_anndata(\n",
    "    mode=\"test\",\n",
    "    normalize=True,\n",
    "    remove_batch_effect=False,\n",
    "    target_hierarchy_level=-1,\n",
    "    preload_subsample_frac=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adamb\\miniconda3\\Lib\\site-packages\\anndata\\_core\\anndata.py:1820: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n"
     ]
    }
   ],
   "source": [
    "hierarchy = {'level1': ['Hematopoietic Stem Cells and Progenitors', 'Hematopoietic Stem Cells and Progenitors', 'Hematopoietic Stem Cells and Progenitors', 'Hematopoietic Stem Cells and Progenitors', 'Monocytes and Dendritic Cells', 'Monocytes and Dendritic Cells', 'Monocytes and Dendritic Cells', 'Monocytes and Dendritic Cells', 'Monocytes and Dendritic Cells', 'T Cells', 'T Cells', 'T Cells', 'T Cells', 'T Cells', 'T Cells', 'T Cells', 'T Cells', 'T Cells', 'T Cells', 'T Cells', 'T Cells', 'T Cells', 'T Cells', 'T Cells', 'T Cells', 'T Cells', 'T Cells', 'T Cells', 'B Cells and Plasma Cells', 'B Cells and Plasma Cells', 'B Cells and Plasma Cells', 'B Cells and Plasma Cells', 'B Cells and Plasma Cells', 'B Cells and Plasma Cells', 'B Cells and Plasma Cells', 'B Cells and Plasma Cells', 'B Cells and Plasma Cells', 'Natural Killer Cells', 'Natural Killer Cells', 'Erythroid Lineage', 'Erythroid Lineage', 'Erythroid Lineage', 'Erythroid Lineage', 'Innate Lymphoid Cells (ILCs)', 'Innate Lymphoid Cells (ILCs)'], 'level2': ['HSC', 'Progenitors', 'Progenitors', 'Progenitors', 'Monocytes', 'Monocytes', 'Dendritic Cells', 'Dendritic Cells', 'Dendritic Cells', 'CD4+ T Cells', 'CD4+ T Cells', 'CD4+ T Cells', 'CD4+ T Cells', 'CD8+ T Cells', 'CD8+ T Cells', 'CD8+ T Cells', 'CD8+ T Cells', 'CD8+ T Cells', 'CD8+ T Cells', 'CD8+ T Cells', 'CD8+ T Cells', 'CD8+ T Cells', 'Special T Cell Types', 'Special T Cell Types', 'Special T Cell Types', 'Special T Cell Types', 'Special T Cell Types', 'Special T Cell Types', 'Naive B Cells', 'Naive B Cells', 'Transitional B Cells', 'B1 B Cells', 'B1 B Cells', 'Plasma Cells', 'Plasma Cells', 'Plasmablasts', 'Plasmablasts', 'NK', 'NK CD158e1+', 'Erythroblasts and Precursors', 'Erythroblasts and Precursors', 'Erythroblasts and Precursors', 'Reticulocytes', 'ILC', 'ILC1'], 'level3': ['HSC', 'Lymph prog', 'G/M prog', 'MK/E prog', 'CD14+ Mono', 'CD16+ Mono', 'cDC1', 'cDC2', 'pDC', 'CD4+ T naive', 'CD4+ T activated', 'CD4+ T activated integrinB7+', 'CD4+ T CD314+ CD45RA+', 'CD8+ T naive', 'CD8+ T CD57+ CD45RO+', 'CD8+ T CD57+ CD45RA+', 'CD8+ T TIGIT+ CD45RO+', 'CD8+ T TIGIT+ CD45RA+', 'CD8+ T CD49f+', 'CD8+ T CD69+ CD45RO+', 'CD8+ T CD69+ CD45RA+', 'CD8+ T naive CD127+ CD26- CD101-', 'MAIT', 'gdT CD158b+', 'gdT TCRVD2+', 'dnT', 'T reg', 'T prog cycling', 'Naive CD20+ B IGKC+', 'Naive CD20+ B IGKC-', 'Transitional B', 'B1 B IGKC+', 'B1 B IGKC-', 'Plasma cell IGKC+', 'Plasma cell IGKC-', 'Plasmablast IGKC+', 'Plasmablast IGKC-', 'NK', 'NK CD158e1+', 'Proerythroblast', 'Erythroblast', 'Normoblast', 'Reticulocyte', 'ILC', 'ILC1'], 'description': ['Hematopoietic Stem Cells', 'Lymphoid progenitors', 'Granulocyte-Macrophage progenitors', 'Megakaryocyte-Erythroid progenitors', 'CD14+ Monocytes', 'CD16+ Monocytes', 'Conventional Dendritic Cells, subtype 1', 'Conventional Dendritic Cells, subtype 2', 'Plasmacytoid Dendritic Cells', 'Naive CD4+ T Cells', 'Activated CD4+ T Cells', 'Activated CD4+ T Cells expressing integrin β7', 'CD4+ T Cells expressing CD314 and CD45RA', 'Naive CD8+ T Cells', 'CD8+ T Cells expressing CD57 and CD45RO', 'CD8+ T Cells expressing CD57 and CD45RA', 'CD8+ T Cells expressing TIGIT and CD45RO', 'CD8+ T Cells expressing TIGIT and CD45RA', 'CD8+ T Cells expressing CD49f', 'CD8+ T Cells expressing CD69 and CD45RO', 'CD8+ T Cells expressing CD69 and CD45RA', 'Naive CD8+ T Cells expressing CD127, not CD26 or CD101', 'Mucosal-Associated Invariant T Cells', 'Gamma Delta T Cells expressing CD158b', 'Gamma Delta T Cells expressing TCRVδ2', 'Double-Negative T Cells', 'Regulatory T Cells', 'Cycling Progenitor T Cells', 'Naive B Cells expressing CD20 and IGKC', 'Naive B Cells expressing CD20, not IGKC', 'Transitional B Cells', 'B1 B Cells expressing IGKC', 'B1 B Cells not expressing IGKC', 'Plasma Cells expressing IGKC', 'Plasma Cells not expressing IGKC', 'Plasmablasts expressing IGKC', 'Plasmablasts not expressing IGKC', 'Natural Killer Cells', 'Natural Killer Cells expressing CD158e1', 'Proerythroblast', 'Erythroblast', 'Normoblast', 'Reticulocyte', 'Innate Lymphoid Cells', 'ILC1']}\n",
    "hierarchy_df = pd.DataFrame(data = hierarchy)\n",
    "test_data.obs = pd.merge(test_data.obs, hierarchy_df, left_on=\"cell_type\", right_on=\"level3\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_NAME = \"latent_omiae\"\n",
    "DATA_PATH = \"../data/latent/latent_omiae_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_NAME = \"omivae_latent\"\n",
    "DATA_PATH = \"../data/latent/omivae_2024-06-15_14-02-51/latent_test.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_NAME = \"babel_latent_adt\"\n",
    "DATA_PATH = \"../data/latent/adam_babel/adam_babel_latent_test_adt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_NAME = \"babel_latent_gex\"\n",
    "DATA_PATH = \"../data/latent/adam_babel/adam_babel_latent_test_gex\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_NAME = \"latent_omiae\"\n",
    "DATA_PATH = \"../data/latent/omiae_simple_2024-06-15_10-32-21/latent_omiae_test.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.obsm[LATENT_NAME] = torch.load(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downstream task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 16\n",
    "DATA = test_data\n",
    "LEVEL = 'level3'\n",
    "HIDDEN_DIM = 16 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adamb\\AppData\\Local\\Temp\\ipykernel_26028\\2057320055.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(DATA.obsm[LATENT_NAME], dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# Extract the labels (cell types)\n",
    "labels = DATA.obs[LEVEL].values\n",
    "\n",
    "# Encode labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X = torch.tensor(DATA.obsm[LATENT_NAME], dtype=torch.float32)\n",
    "y = torch.tensor(labels_encoded, dtype=torch.long)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DataLoader for training and testing sets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.building_blocks import Block\n",
    "\n",
    "\n",
    "class ClassificationModel(pl.LightningModule):\n",
    "    def __init__(self, latent_dim, num_classes, hidden_dim, do_batch_norm=False):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes),\n",
    "        )\n",
    "        self.loss = nn.CrossEntropyLoss()  # Adjust\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.layers(z)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.log(\n",
    "            \"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.log(\n",
    "            \"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Define optimizer here\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(), lr=0.001\n",
    "        )  # Adjust learning rate\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name   | Type             | Params\n",
      "--------------------------------------------\n",
      "0 | layers | Sequential       | 3.7 K \n",
      "1 | loss   | CrossEntropyLoss | 0     \n",
      "--------------------------------------------\n",
      "3.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.7 K     Total params\n",
      "0.015     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f7e12dbd1724f568c2c17f7351c9fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adamb\\miniconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\adamb\\miniconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b78ba83ab2489391170887738ae0a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b5284a81a74017854e9767d318558a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6279afa095b245ff8fc73eb0fdba7218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46caad7a4be84674aebb962e269aad80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175e48e609694476b07a78d1a9da401a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da3420e5ea1042b58781715eaee59ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee470efb46c54cdba7e67b72d555baac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31245a65d6244e138ff8b15b3ee8ba57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f994438717c24b0595d9ae1a6ecf2aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5094ffeb93243a1ba51228daad767a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f39e37a38a7444b1a4001898a534023c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "output_dim = len(np.unique(labels_encoded))\n",
    "\n",
    "model = ClassificationModel(latent_dim=LATENT_DIM,\n",
    "                            num_classes=output_dim,\n",
    "                            hidden_dim=HIDDEN_DIM,\n",
    "                            do_batch_norm=False)\n",
    "\n",
    "# Define a trainer\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath=f'logs/classification_checkpoints/{LATENT_NAME}',\n",
    "    filename='model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,\n",
    "    mode='min',\n",
    ")\n",
    "logger = TensorBoardLogger(\"./logs\", name=f\"{LATENT_NAME}_classifing_logs\")\n",
    "trainer = pl.Trainer(max_epochs=10, callbacks=[checkpoint_callback])\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 77.64%\n"
     ]
    }
   ],
   "source": [
    "# Loading the best model for evaluation\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "best_model = ClassificationModel.load_from_checkpoint(\n",
    "    best_model_path,\n",
    "    latent_dim=LATENT_DIM,\n",
    "    num_classes=output_dim,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    do_batch_norm=False).cpu()\n",
    "\n",
    "# Evaluate the model on test set\n",
    "best_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = best_model(data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = best_model(X_test)\n",
    "_, predicted = torch.max(output.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(y_true, y_pred):\n",
    "    balanced_accuracy_score = sklearn.metrics.balanced_accuracy_score(y_true, y_pred)\n",
    "    accuracy_score = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
    "    f1_score = sklearn.metrics.f1_score(y_true, y_pred, average=\"macro\")\n",
    "    \n",
    "    print(f\"Accuracy score: {accuracy_score}\")\n",
    "    print(f\"Balanced accuracy score: {balanced_accuracy_score}\")\n",
    "    print(f\"F1 score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.7763769077637691\n",
      "Balanced accuracy score: 0.6210705492163321\n",
      "F1 score: 0.6098491773272356\n"
     ]
    }
   ],
   "source": [
    "print_metrics(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
